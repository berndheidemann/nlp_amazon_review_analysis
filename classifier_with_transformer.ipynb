{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle=False\n",
    "kaggle_path='/kaggle/input/amazon-deutsch-review-dataset/Amazon-Deutsch-Dataset.csv'\n",
    "local_path='Amazon-Deutsch-Dataset.csv'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2934</th>\n",
       "      <td>Bei mir waren nach jedem Bildschirm-Timeout so...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511</th>\n",
       "      <td>Das nötige Setup für Klingeltöne, Lautsprecher...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>Die guten Rezensionen erweckten hohe Erwartung...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>....was will man für 8€ erwarten? Meine Sony i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>Sehr enttäuschend. Haben nach knapp 2 Wochen s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content  rating\n",
       "2934  Bei mir waren nach jedem Bildschirm-Timeout so...       0\n",
       "1511  Das nötige Setup für Klingeltöne, Lautsprecher...       0\n",
       "292   Die guten Rezensionen erweckten hohe Erwartung...       0\n",
       "901   ....was will man für 8€ erwarten? Meine Sony i...       0\n",
       "1017  Sehr enttäuschend. Haben nach knapp 2 Wochen s...       0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df=pd.read_csv(kaggle_path if kaggle else local_path)\n",
    "df = df[[\"content\", \"rating\"]]\n",
    "df.rating= df.rating.str[0]\n",
    "df = df.dropna()\n",
    "\n",
    "df.rating = df.rating.astype(int)\n",
    "df.rating = df.rating.apply(lambda x: 1 if x>3 else 0)\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2144\n",
       "1    1265\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Tokenizer-Funktion (einfaches Beispiel)\n",
    "def tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Vokabular erstellen\n",
    "def build_vocab(texts, vocab_size):\n",
    "    word_counts = {}\n",
    "    for text in texts:\n",
    "        tokens = tokenizer(text)\n",
    "        for token in tokens:\n",
    "            if token in word_counts:\n",
    "                word_counts[token] += 1\n",
    "            else:\n",
    "                word_counts[token] = 1\n",
    "\n",
    "    sorted_vocab = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    sorted_vocab = sorted_vocab[:vocab_size-1]\n",
    "    word_to_idx = {word: idx+1 for idx, (word, _) in enumerate(sorted_vocab)}\n",
    "    word_to_idx['<unk>'] = 0\n",
    "    return word_to_idx\n",
    "\n",
    "# Texte in Sequenzen von Wortindizes umwandeln\n",
    "def text_to_indices(text, word_to_idx):\n",
    "    tokens = tokenizer(text)\n",
    "    indices = [word_to_idx[token] if token in word_to_idx else 0 for token in tokens]\n",
    "    return indices\n",
    "\n",
    "# Hyperparameter\n",
    "vocab_size = 10000\n",
    "word_count = 200\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "num_classes = 2\n",
    "batch_size = 32\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "# Aufteilung in Trainings- und Testdaten\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Erstellung des Vokabulars\n",
    "texts = train_df['content'].tolist()\n",
    "word_to_idx = build_vocab(texts, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mein altes Handy gab den Geist auf - wie das oft so der Fall ist, vor dem verlängerten Wochenende. Also musste ich noch schnell los und mir ein neues besorgen (daher stammt es auch nicht von Amazon). Das klappte - aber dann trat das nächste Problem auf ... ich hatte vorher (im Galaxy S4 Mini) eine MikroSIM und brauchte jetzt eine NanoSIM. Also musste ich eine halbe Stunde vor Landeschluss noch in den T-Shop, um mir eine neue SIM-Karte zu besorgen.\\n\\nAlso - Merkposten Nr. 1 (wenn man von älteren Handys kommt) - neue NanoSIM besorgen!\\n\\nIch bin nicht (mehr) der Ansicht, dass ich immer das allerneuste Handy haben muss. Ein Gerät, das zur \"vorletzten\" Generation gehört (die neuen Galaxy S8er Handys sind ja raus) reicht für mich vollkommen aus (und ist ein Riesenfortschritt zum S4 Mini).\\n\\nWas gefällt mir?\\n- das sehr schöne Display. Gute Größe (für meinen Geschmack) - bietet viel Platz, gute Auflösung, schöne Farben - und ist jedenfalls für mich mit einer Handy bedienbar (habe allerdings auch große Pranken).\\n- 32GB interner Speicher sind schon mal ein Wort und viel besser als die 8GB des vorherigen Geräts (die Erweiterbarkeit über Speicherkarten ist ebenfalls sehr gut)\\n- die Kamera gefällt mir bisher sehr gut, die Bilder sind mit (meinem) Vorgängermodell nicht vergleichbar. Auch ältere Kompaktkameras kommen da nicht hjinterher. Das die Fotos trotz alledem natürlich nicht mit Aufnahmen einer Spiegelreflexkamera vergleichbar sind, sollte sich von selbst verstehen!\\n- die Leistung des Geräts ist für meine Ansprüche vollkommen ausreichend (bzw vermutlich immer noch drastisch überdimensioniert). Ich nutze Handys nicht zum Zocken und auch nicht zum Videoschauen. Bei mir stehen tatsächlich telefonieren, , Messagingdienste wie WhattsApp, Internetnutzung, hin und wieder mal Fotos und vielleicht Musikhören im Zug an erster Stelle - und dafür funktioniert es perfekt. Apps laufen ruckelfrei und ohne Verzögerung.\\n- die Auflasdung funktioiniert über die normalen USB-Buchsen. Es wird kein proprietärer Stecker gebraucht, die gesamte vorhandene Ladeinfrastruktur kann weiter genutzt werden. Was ich demnächst noch ausprobieren werde, ist das induktive Laden, das entsprechende Ladegerät ist bestellt, das kommt sicher noch.\\n- das Always-on-Display. Die Uhrzeit wird durchgehend angezeigt (ich benutze schon seit längerem keine Armbanduhren mehr, daher ist das Handy bei mir auch immer Uhr) und Nachrichten werden durch ein entsprechendes Symbol immer dargestellt.\\n\\nAlso Aoftware war auf meinem Gerät entweder das Android7 bereits installiert oder er hat direkt nach der Inbetriebnahme im Zusammenhang mit der Einrichtung des Geräts auf Android7 upgedatet! Auch insoweit gibt es also nichts zu beanstanden.\\n\\nWas mir nicht so gut gefällt ist die Tatsache, dass es keinen wechselbaren Akku gibt. Das ist ein leidiges Thema, das aber fast alle modernen Handy betrifft ... und vermutlich hier auch u.a. der Wasserdichtigkeit geschuldet ist. Dieselbe allerdings habe ich bisher nicht ausprobiert und wäre auch dankbar, wenn ich das nicht tun müsste.\\nZurück zum Akku - bisher gefällt mir die Akkuleistung recht gut. Ich komme ohne weiteres durch den Tag ohne mir größere Gedanken machen zu müssen, bei etwas weniger Nutzung sind es auch ohne weiteres fast zwei Tage, die der Akku reicht.\\n\\nAlles in allem bin ich sehr zufrieden mit dem Gerät und würde es mir wieder kaufen. Vermissen tue ich nichts und ich wüsste auch aktuell nicht, was ein neueres Smartphone noch besser können sollte.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx[\"Handy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[307, 523, 47, 483, 14, 412, 18, 33, 46, 3]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_indices(texts[0], word_to_idx)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  21,   69,   16,  179,   10,   22,  122, 4016,   67, 1480,   95,  122,\n",
      "        1466,   32,    9, 6146,    0,    4,   27,  163,    0,  212,   21,  170,\n",
      "           8, 1592,   55,  285,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "class AmazonDataset(Dataset):\n",
    "    def __init__(self, df, word_count=500, vocab_size=10000):\n",
    "        self.df = df\n",
    "        self.word_count = word_count\n",
    "        self.vocab_size = vocab_size\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        x= self.df.iloc[idx][\"content\"]\n",
    "        y= self.df.iloc[idx][\"rating\"]\n",
    "        y = int(y)\n",
    "        x = text_to_indices(x, word_to_idx)\n",
    "        # we need this because we need to have a fixed size input\n",
    "        if len(x) > self.word_count:\n",
    "            x=x[:self.word_count]\n",
    "        else:\n",
    "        # pad with zeros, in case the text is shorter than word_count\n",
    "            x.extend([0]*(self.word_count-len(x)))\n",
    "        x = torch.tensor(x)\n",
    "        y= torch.tensor(y, dtype=torch.long)\n",
    "        return x, y\n",
    "    \n",
    "amazon_dataset = AmazonDataset(df, word_count=50, vocab_size=vocab_size)\n",
    "x,y=amazon_dataset[0]\n",
    "print(x)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ich bin sehr zufrieden mit dem iPhone 11. Der Wechsel vom iPhone 6s war ein riesiger <unk> der sich allerdings <unk> hat. Ich würde es jederzeit wieder kaufen. <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "def indices_to_text(indices, idx_to_word):\n",
    "    tokens = [idx_to_word[idx.item()] for idx in indices]\n",
    "    text = \" \".join(tokens)\n",
    "    return text\n",
    "\n",
    "indices_to_text(x, idx_to_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding-Funktion für Batch\n",
    "def custom_collate_fn(batch):\n",
    "    inputs, labels = zip(*batch)  # this means: separate inputs and labels from the batch\n",
    "    inputs = [torch.tensor(text, dtype=torch.long) for text in inputs]\n",
    "    inputs = pad_sequence(inputs, batch_first=True, padding_value=0) \n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50])\n",
      "torch.Size([32])\n",
      "<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yl/qjs6b9wn4zx7nh630c4my9lw0000gn/T/ipykernel_42651/3151703939.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = [torch.tensor(text, dtype=torch.long) for text in inputs]\n"
     ]
    }
   ],
   "source": [
    "# test custom_collate_fn\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(amazon_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "inputs, labels = next(iter(train_loader))\n",
    "print(inputs.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "print(indices_to_text(inputs[2], idx_to_word))\n",
    "print(labels[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:  10000\n",
      "embedding_dim:  128\n",
      "hidden_dim:  256\n",
      "num_layers:  2\n",
      "num_classes:  2\n",
      "word_count:  200\n",
      "batch_size:  32\n",
      "torch.Size([32, 200])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yl/qjs6b9wn4zx7nh630c4my9lw0000gn/T/ipykernel_42651/3151703939.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = [torch.tensor(text, dtype=torch.long) for text in inputs]\n"
     ]
    }
   ],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_classes, word_count):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=4,  # Number of attention heads\n",
    "            dim_feedforward=hidden_dim,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(self.transformer_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(word_count*embedding_dim, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.batch_norm = nn.BatchNorm1d(word_count*embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        #print(\"embedded shape: \", embedded.shape)\n",
    "        embedded = embedded.permute(1, 0, 2)  # Shape: (word_count, batch_size, embedding_dim)\n",
    "        #print(\"embedded 2 shape: \", embedded.shape)\n",
    "\n",
    "        transformer_output = self.transformer(embedded)\n",
    "        #print(\"transformer_output shape: \", transformer_output.shape)\n",
    "        transformer_output = transformer_output.permute(1, 0, 2)  # Back to (batch_size, word_count, embedding_dim)\n",
    "        #print(\"transformer_output 2 shape: \", transformer_output.shape)\n",
    "        transformer_output = transformer_output.contiguous().view(transformer_output.size(0), -1)\n",
    "        out=self.batch_norm(transformer_output)\n",
    "        out=self.dropout(out)\n",
    "        #print(\"transformer_output 3 shape: \", transformer_output.shape)\n",
    "        out = self.fc(transformer_output)\n",
    "        #print(\"out shape: \", out.shape)\n",
    "        out = self.relu(out)\n",
    "        #print(\"final out  shape: \", out.shape)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "print(\"vocab_size: \", vocab_size)\n",
    "print(\"embedding_dim: \", embedding_dim)\n",
    "print(\"hidden_dim: \", hidden_dim)\n",
    "print(\"num_layers: \", num_layers)\n",
    "print(\"num_classes: \", num_classes)\n",
    "print(\"word_count: \", word_count)\n",
    "print(\"batch_size: \", batch_size)\n",
    "\n",
    "# test model\n",
    "train_dataset = AmazonDataset(train_df, word_count, vocab_size)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "x, y = next(iter(train_loader))\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "model = TransformerClassifier(vocab_size, embedding_dim, hidden_dim, num_layers, num_classes, word_count)\n",
    "out = model(x)\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "if kaggle==False:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def calcAccuracy(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6be3116c5b045f6b131ae7deaec4c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yl/qjs6b9wn4zx7nh630c4my9lw0000gn/T/ipykernel_42651/3151703939.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = [torch.tensor(text, dtype=torch.long) for text in inputs]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 61.401256024837494\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/berndheidemann/Developer/data_science/text/amazon_review_analysis/classifier_with_transformer.ipynb Zelle 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/berndheidemann/Developer/data_science/text/amazon_review_analysis/classifier_with_transformer.ipynb#X15sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/berndheidemann/Developer/data_science/text/amazon_review_analysis/classifier_with_transformer.ipynb#X15sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/berndheidemann/Developer/data_science/text/amazon_review_analysis/classifier_with_transformer.ipynb#X15sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/berndheidemann/Developer/data_science/text/amazon_review_analysis/classifier_with_transformer.ipynb#X15sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m#print(\"outputs.shape\", outputs)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/berndheidemann/Developer/data_science/text/amazon_review_analysis/classifier_with_transformer.ipynb#X15sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m#print(\"labels.shape\", labels)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/berndheidemann/Developer/data_science/text/amazon_review_analysis/classifier_with_transformer.ipynb#X15sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/berndheidemann/Developer/data_science/text/amazon_review_analysis/classifier_with_transformer.ipynb Zelle 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/berndheidemann/Developer/data_science/text/amazon_review_analysis/classifier_with_transformer.ipynb#X15sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m embedded \u001b[39m=\u001b[39m embedded\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# Shape: (word_count, batch_size, embedding_dim)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/berndheidemann/Developer/data_science/text/amazon_review_analysis/classifier_with_transformer.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m#print(\"embedded 2 shape: \", embedded.shape)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/berndheidemann/Developer/data_science/text/amazon_review_analysis/classifier_with_transformer.ipynb#X15sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m transformer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(embedded)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/berndheidemann/Developer/data_science/text/amazon_review_analysis/classifier_with_transformer.ipynb#X15sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m#print(\"transformer_output shape: \", transformer_output.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/berndheidemann/Developer/data_science/text/amazon_review_analysis/classifier_with_transformer.ipynb#X15sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m transformer_output \u001b[39m=\u001b[39m transformer_output\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# Back to (batch_size, word_count, embedding_dim)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/torch/nn/modules/transformer.py:315\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_causal \u001b[39m=\u001b[39m make_causal\n\u001b[1;32m    314\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 315\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, is_causal\u001b[39m=\u001b[39;49mis_causal, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask_for_layers)\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    318\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mto_padded_tensor(\u001b[39m0.\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/torch/nn/modules/transformer.py:598\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    596\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[1;32m    597\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 598\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[39m=\u001b[39;49mis_causal))\n\u001b[1;32m    599\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[1;32m    601\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/torch/nn/modules/transformer.py:606\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[1;32m    605\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 606\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[1;32m    607\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    608\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m    609\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, is_causal\u001b[39m=\u001b[39;49mis_causal)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    610\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/torch/nn/modules/activation.py:1229\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1215\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1216\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1217\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1226\u001b[0m         average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1227\u001b[0m         is_causal\u001b[39m=\u001b[39mis_causal)\n\u001b[1;32m   1228\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1229\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1230\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1231\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1232\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1233\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1234\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1235\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m   1236\u001b[0m         need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1237\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m   1238\u001b[0m         average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights,\n\u001b[1;32m   1239\u001b[0m         is_causal\u001b[39m=\u001b[39;49mis_causal)\n\u001b[1;32m   1240\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1241\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/torch/nn/functional.py:5402\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5399\u001b[0m k \u001b[39m=\u001b[39m k\u001b[39m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[1;32m   5400\u001b[0m v \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[0;32m-> 5402\u001b[0m attn_output \u001b[39m=\u001b[39m scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n\u001b[1;32m   5403\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(bsz \u001b[39m*\u001b[39m tgt_len, embed_dim)\n\u001b[1;32m   5405\u001b[0m attn_output \u001b[39m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "num_epochs = 100\n",
    "\n",
    "# Daten laden und Dataloader erstellen\n",
    "train_dataset = AmazonDataset(train_df, word_count, vocab_size)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "test_dataset = AmazonDataset(test_df, word_count, vocab_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "\n",
    "# Modell initialisieren\n",
    "model = TransformerClassifier(vocab_size, embedding_dim, hidden_dim, num_layers, num_classes, word_count)\n",
    "model=model.to(device)\n",
    "\n",
    "# Optimizer und Loss-Funktion\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Trainingsschleife\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        #print(\"outputs.shape\", outputs)\n",
    "        #print(\"labels.shape\", labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss}\")\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(\"Train Accuracy: \", calcAccuracy(model, train_loader))\n",
    "        print(\"Test Accuracy: \", calcAccuracy(model, test_loader))\n",
    "        \n",
    "\n",
    "# Evaluierung auf Testdaten\n",
    "model.eval()\n",
    "\n",
    "\n",
    "print(\"Accuracy: \", calcAccuracy(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 200])\n",
      "outputs tensor([[0.0000, 0.1023, 1.2175, 0.3537, 0.0000],\n",
      "        [0.0000, 0.1700, 1.0861, 0.3672, 0.0000]])\n",
      "predicted tensor([2, 2])\n"
     ]
    }
   ],
   "source": [
    "text = [\"Das Handy ist sehr gut. Wirklich empfehlenswert!\", \"Das Handy ist sehr schlecht. Nicht empfehlenswert\"]\n",
    "\n",
    "text = [text_to_indices(t, word_to_idx) for t in text]\n",
    "\n",
    "def padd_text_batch(batch):\n",
    "    for t in batch:\n",
    "        if len(t) < word_count:\n",
    "            t.extend([0]*(word_count-len(t)))\n",
    "    return torch.tensor(batch, dtype=torch.long)\n",
    "\n",
    "text = padd_text_batch(text)\n",
    "device = torch.device(\"cpu\")\n",
    "text = text.to(device)\n",
    "# padd with zeros\n",
    "print(text.shape)\n",
    "model=model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(text)\n",
    "    print(\"outputs\", outputs)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    print(\"predicted\", predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
